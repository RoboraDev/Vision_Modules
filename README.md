# Robora Vision Modules

Robora Vision Modules is a Python library that provides plug and play vision skills reusable across Robora demos and inside robot ‚Äúbrains.‚Äù

This repository is part of Robora‚Äôs initiative to collaborate with students and researchers from different universities. The goal is to expand the reach of robotics combined with blockchain and create an open environment where knowledge, tools, and real world applications can grow through collaboration.

If you are a student, researcher, or developer, you are welcome to contribute. Fork the repository, make improvements, and submit a pull request. Together, we can advance robotics x blockchain and push forward the adoption of physical AI.

---

## ‚ú® Features
- **Object Detection** (YOLOv11 wrapper)
- **Object Tracking**  
- **Image Segmentation** (SAM-lite or similar lightweight segmenter)  
- **Marker / Barcode Detection** (ArUco or QR)
- **Pose Estimation**
- **Unified Python API** for simple usage  
- **Command-line tools (CLI)** for quick demos  
- **Tiny evaluation script** for COCO-style datasets  

---

## üìÇ Repository Structure
```
Vision_Modules/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ rvm/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ api.py                # unified high-level API
‚îÇ   ‚îú‚îÄ‚îÄ cli/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ detect.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ eval_coco.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ markers.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ segment.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pose.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ track.py
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ types.py          # dataclasses for boxes, masks, markers
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ visualize.py      # drawing utilities
‚îÇ   ‚îú‚îÄ‚îÄ detect/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ yolo.py           # detection wrapper
‚îÇ   ‚îú‚îÄ‚îÄ segment/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sam_lite.py       # segmentation wrapper
‚îÇ   ‚îú‚îÄ‚îÄ markers/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aruco.py          # marker detection
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ barcodes.py       # QR Codes & Bar Codes detection
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pose.py
‚îÇ   ‚îú‚îÄ‚îÄ track/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄtracker.py         # tracking wrapper
‚îÇ   ‚îî‚îÄ‚îÄ io/
‚îÇ       ‚îú‚îÄ‚îÄ loader.py         # image, video, webcam loading
‚îÇ       ‚îî‚îÄ‚îÄ writer.py         # save JSON + annotated media
|
‚îú‚îÄ‚îÄ demos/
‚îÇ   ‚îú‚îÄ‚îÄ detect_webcam.py
‚îÇ   ‚îú‚îÄ‚îÄ detect_video.py
‚îÇ   ‚îú‚îÄ‚îÄ segment_image.py
‚îÇ   ‚îú‚îÄ‚îÄ markers_image.py
‚îÇ   ‚îú‚îÄ‚îÄ markers_pose.py
‚îÇ   ‚îî‚îÄ‚îÄ tracking.py
‚îÇ
‚îú‚îÄ‚îÄ eval/
‚îÇ   ‚îî‚îÄ‚îÄ coco_eval.py       # detection metrics + report.html
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_api_smoke.py
‚îÇ   ‚îú‚îÄ‚îÄ test_visualize.py
‚îÇ   ‚îú‚îÄ‚îÄ test_pose.py
‚îÇ   ‚îî‚îÄ‚îÄ test_coco_eval.py
‚îÇ
‚îú‚îÄ‚îÄ samples/
‚îÇ   ‚îú‚îÄ‚îÄ shelf.jpg
‚îÇ   ‚îî‚îÄ‚îÄ tags.png
‚îÇ
‚îî‚îÄ‚îÄ .github/
    ‚îî‚îÄ‚îÄ workflows/
        ‚îî‚îÄ‚îÄ ci.yml            # CI pipeline: run tests on push
```

---

## üöÄ Installation

We would recommend that you install the project inside a virtual environment to avoid dependency conflicts.

#### 1. Clone the repository
```bash
git clone https://github.com/RoboraDev/Vision_Modules
cd Vision_Modules
```
#### 2. Create and activate a virtual environment
##### Create virtual environment
```bash
python3.11 -m venv venv_rvm
```

##### Activate (Linux/Mac)
```bash
source venv_rvm/bin/activate
```

##### Activate (Windows)
```bash
venv_rvm\Scripts\activate
```

#### 3. Install dependencies
```bash
pip install -r requirements.txt
```

#### 4. Install the package in editable mode
```bash
pip install -e .
```

#### üî• Quick Install (alternative)

If you already have the required dependencies installed, you can skip steps 2‚Äì3 and install directly:
```bash
pip install -e .
```
---

## üì¶ Requirements
- torch >= 2.2  
- ultralytics >= 8.1  
- opencv-python >= 4.9  
- numpy >= 1.26  
- matplotlib >= 3.8  
- pyzbar >= 0.1.9 (or `opencv-contrib-python` if using ArUco)  
- pycocotools >= 2.0.7  

---

## üßë‚Äçüíª Usage
### CLI Commands
```bash
rvm-detect --source path_or_webcam --model yolo11n.pt --out results/
rvm-track --source path_or_webcam  --tracker ultralytics_or_iou --out results/
rvm-segment --source images_dir --out results/
rvm-markers --source images_dir --out results/
rvm-pose --image path_to_image --calib path_to_cali_results --marker-size 0.05 --out results/
rvm-eval-coco --images images_dir --ann annotations.json --out reports/
```

### Python API
You can also use **Vision Modules** directly in Python without the CLI.

#### 1. Object Detection
```python
from rvm.api import detect

results = detect(
    source="path/to/images_or_video",   # file, folder, or webcam index
    model="yolo11n.pt",                 # YOLO model checkpoint
    out_dir="results/"                  # output directory
)
print(results)
```

#### 2. Object Tracking
```
from rvm.api import track

tracks = track(
    source="path/to/video.mp4",    # video file or webcam index
    tracker="iou",                 # "iou" (lightweight) or "ultralytics" (deep learning)
    model="yolo11n.pt",            # required if using "ultralytics"
    out_dir="results/"
)
print(tracks)
```

#### 3. Segmentation
```python
from rvm.api import segment

masks = segment(
    image_path= "path/to/images_dir",
    out_dir="results/"
)
print(masks[0].shape)
```


#### 4. Markers
```python
from rvm.api import markers

output = markers(
    image_path="path/to/images_dir",
    out_dir="results/"
)
print(output)
```

#### 5. Pose Estimation
```python
from rvm.api import detect_marker_poses

poses = detect_marker_poses(
    image_path="path/to/iimages_dir",   
    camera_calib="data/camera_calib.yaml",         
    marker_size=0.05,                          
    out="results/"                                
)
print(poses)
```
  
#### 6. COCO Evaluation
```python
from rvm.api import coco_eval

metrics = coco_eval(
    pred_file="preds.json",          # predictions in COCO format
    ann_file="annotations.json",     # ground-truth annotations
    out_dir="reports/"
)
print(metrics)
```


---

## üé• Demos
We provide simple demo scripts for quick testing:

- `demos/detect_video.py`  ‚Üí run YOLO detection from video
- `demos/detect_webcam.py` ‚Üí run YOLO detection live from webcam
- `demos/tracking.py`      ‚Üí run object tracking live from webcam
- `demos/detect_video.py`  ‚Üí detect objects in video, save annotated MP4 + JSON  
- `demos/segment_image.py` ‚Üí run SAM-lite segmentation on an image  
- `demos/markers_image.py` ‚Üí detect QR/ArUco markers in image
- `demos/marker_pose.py`   ‚Üí run pose estimation

Example:
```bash
python demos/detect_webcam.py
python demos/tracking.py
```

---

## üìä Evaluation
Run COCO-style evaluation on predictions:
```bash
rvm-eval-coco --images path/to/images_dir --ann annotations.json --out reports/
```

This will output:
- Precision (AP@[0.5:0.95])
- Recall (AR@100)
- report.html (human-readable report)
- pr_curve.png (precision‚Äìrecall curve)

---

## ‚úÖ Tests & CI
We use pytest for testing and GitHub Actions for continuous integration.
Run all tests locally:
```bash
pytest -v
```
Tests include:
- Unit tests for each API function
- Integration tests for visualization
- Evaluation tests with minimal COCO-format data
  
CI automatically runs these tests on every pull request.

## üìå Roadmap
- Updating  

---

## ü§ù Collaboration  

This repository is built with collaboration in mind. Robora is working closely with students, universities, and research groups to advance robotics and blockchain together.  

### How to contribute  
1. Fork this repository  
2. Create a new branch for your feature or fix  
3. Commit your changes  
4. Push your branch  
5. Open a pull request  

All contributions are welcome, whether through research ideas, code improvements, documentation, or new demos.  

---

üåê Community and Links

[Website](https://robora.xyz)

[X](https://x.com/userobora)

[Telegram](https://t.me/roboratg)

[Medium](https://robora.medium.com)

---

## üë• Authors
Maintained by **Robora**.  
Contributor: [@ncquy](https://github.com/ncquy), [@TianleiZhou](https://github.com/TianleiZhou), *Updating...* 
